# Data Generation Flow: How Computed Features are Created

## Overview
Data flows through multiple stages: **Raw Source → Database → Feature Computation → Enriched Storage**

---

## Stage 1: Raw Data Ingestion (Pulled from Source)

### Sources
- **Stocks/ETFs:** Polygon.io API
- **Crypto:** Binance API, Polygon.io

### Data Pulled
```
open, high, low, close, volume  (all assets)
open_interest, funding_rate, liquidations_long, liquidations_short, taker_buy_volume, taker_sell_volume  (crypto only)
```

### How It Gets In
1. **Scheduler** (`src/scheduler.py`) runs daily
2. Calls **Polygon.io** or **Binance** APIs
3. Data is stored in raw `market_data` table
4. Logged in `enrichment_fetch_log` table

---

## Stage 2: Feature Computation (Generated by System)

### Primary Computation Engine: `src/quant_engine/quant_features.py`

This is where all technical indicators are calculated from raw OHLCV data.

#### **A. Return Calculations**
```python
# Lines 82-107: compute_returns()
return_1h  = log(close / close[-1 hour ago])
return_1d  = log(close / close[-1 day ago])
```
- **Method:** Log returns (more stable than simple returns)
- **Input:** Historical close prices
- **Output:** Decimal percentage (e.g., 0.025 = 2.5%)

#### **B. Volatility Calculations**
```python
# Lines 110-134: compute_volatility()
volatility_20 = std(returns[-20 periods:]) * sqrt(252)
volatility_50 = std(returns[-50 periods:]) * sqrt(252)
```
- **Method:** Rolling standard deviation of returns
- **Annualization:** Multiply by √252 (trading days in a year)
- **Input:** 20 or 50 periods of close prices
- **Output:** Annualized volatility percentage

#### **C. ATR (Average True Range)**
```python
# Lines 137-158: compute_atr()
true_range = max(high - low, abs(high - close[-1]), abs(low - close[-1]))
atr = EMA(true_range, 14-period)
```
- **Method:** 14-period Exponential Moving Average of True Range
- **True Range:** Captures gaps and limits
- **Input:** High, Low, Close prices
- **Output:** Average volatility measured in price units

#### **D. Trend Analysis**
```python
# Computed from price movement
trend_direction = 'up' if close > EMA(20) else 'down' or 'neutral'
market_structure = classify based on recent pivots
rolling_volume_20 = SMA(volume, 20 periods)
```

### Crypto-Specific Features: `src/services/data_enrichment_service.py`

#### **Current Status: Schema-defined but Not Yet Implemented**

These columns exist in the database but calculation logic needs to be added:

```python
# Would compute:
delta = liquidations_long - liquidations_short
# Interpretation: Positive = more longs liquidated (bearish), Negative = more shorts liquidated (bullish)

buy_sell_ratio = taker_buy_volume / taker_sell_volume
# Interpretation: > 1.0 = more buying pressure, < 1.0 = more selling pressure

liquidation_intensity = (liquidations_long + liquidations_short) / (total_volume / 100)
# Normalized liquidation pressure

volume_spike_score = (volume - rolling_volume_20) / rolling_volume_20
# How abnormal is the current volume

long_short_ratio = liquidations_long / liquidations_short
# Direct ratio of long vs short liquidations

funding_rate_percentile = percentile_rank(funding_rate, 252-day history)
# Where is current funding rate in historical distribution

exchange_inflow = net_inflow_to_centralized_exchanges
# Sentiment indicator (on-chain data)

open_interest_change = (open_interest - open_interest[-1 period]) / open_interest[-1 period]
# Rate of change in open positions
```

---

## Stage 3: Data Quality & Validation

### Quality Flags Generated
```python
# Validation Service checks:
is_validated          = passed all quality checks
quality_score         = 0.0-1.0 (completeness, gaps, anomalies)
validation_notes      = text explanation of issues
gap_detected          = missing expected data in period
volume_anomaly        = unusual volume spike or absence
data_completeness     = % of expected fields present
```

### Tracking Tables
- `enrichment_fetch_log` - Records when data was pulled from source
- `enrichment_compute_log` - Records when features were computed
- `data_quality_metrics` - Daily aggregates of quality metrics
- `enrichment_status` - Current freshness and error state

---

## Stage 4: Storage in market_data_v2

### When Data is Inserted
```
UPSERT ON CONFLICT (symbol, asset_class, timeframe, timestamp, revision)
```

### What Gets Stored
**Row = 49 Columns:**

1. **Raw OHLCV** (5 cols): open, high, low, close, volume
2. **Crypto Raw** (6 cols): open_interest, funding_rate, liquidations_long, liquidations_short, taker_buy_volume, taker_sell_volume
3. **Universal Features** (8 cols): return_1h, return_1d, volatility_20, volatility_50, atr, trend_direction, market_structure, rolling_volume_20
4. **Crypto Features** (8 cols): delta, buy_sell_ratio, liquidation_intensity, volume_spike_score, long_short_ratio, funding_rate_percentile, exchange_inflow, open_interest_change
5. **Quality Tracking** (7 cols): source, is_validated, quality_score, validation_notes, gap_detected, volume_anomaly, data_completeness
6. **Versioning** (2 cols): revision, amended_from
7. **Timestamps** (3 cols): fetched_at, computed_at, updated_at
8. **Identification** (5 cols): id, symbol, asset_class, timeframe, timestamp

### Example: Bitcoin 1H Candle
```
Input from Binance API:
  symbol=BTC, asset_class=crypto, timeframe=1h, timestamp=2025-11-13T10:00:00Z
  open=95000, high=96500, low=94800, close=96200, volume=1250000
  open_interest=2450000000, funding_rate=0.000125, liquidations_long=45000000, liquidations_short=38000000
  taker_buy_volume=650000, taker_sell_volume=600000

Processing Steps:
1. Data validation → is_validated=true, quality_score=0.98, gap_detected=false, volume_anomaly=false
2. Feature computation:
   - return_1h = log(96200/96100) = 0.00104 (0.104%)
   - return_1d = log(96200/94500) = 0.0179 (1.79%)
   - volatility_20 = std(20 hourly returns) * sqrt(252) = 0.287 (28.7%)
   - atr = 14-period EMA(true_range) = 850
   - delta = 45000000 - 38000000 = 7000000 (bullish, more shorts liquidated)
   - buy_sell_ratio = 650000 / 600000 = 1.083 (buying pressure)
   - liquidation_intensity = (45000000 + 38000000) / (1250000/100) = 6640
3. Metadata generation:
   - id=auto_increment, source=binance, fetched_at=2025-11-13T10:15:00Z, computed_at=2025-11-13T10:15:30Z

Output Row:
INSERT INTO market_data_v2 VALUES (
  id=12345678, symbol='BTC', asset_class='crypto', timeframe='1h', timestamp='2025-11-13T10:00:00Z',
  open=95000, high=96500, low=94800, close=96200, volume=1250000,
  open_interest=2450000000, funding_rate=0.000125, liquidations_long=45000000, liquidations_short=38000000,
  taker_buy_volume=650000, taker_sell_volume=600000,
  return_1h=0.00104, return_1d=0.0179, volatility_20=0.287, volatility_50=0.294, atr=850,
  trend_direction='up', market_structure='bullish', rolling_volume_20=1180000,
  delta=7000000, buy_sell_ratio=1.083, liquidation_intensity=6640, volume_spike_score=0.059,
  long_short_ratio=1.184, funding_rate_percentile=68.5, exchange_inflow=12500000, open_interest_change=0.0045,
  source='binance', is_validated=true, quality_score=0.98, validation_notes=NULL,
  gap_detected=false, volume_anomaly=false, data_completeness=1.0,
  revision=1, amended_from=NULL, fetched_at='2025-11-13T10:15:00Z', 
  computed_at='2025-11-13T10:15:30Z', updated_at='2025-11-13T10:15:30Z'
)
```

---

## Data Generation Pipeline: Full Flow

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. SCHEDULED JOB (src/scheduler.py runs daily)                  │
│    - Load tracked symbols from database                          │
│    - For each symbol × timeframe combination:                    │
│      - Call Polygon/Binance API                                 │
│      - Fetch: open, high, low, close, volume                    │
│      - (Crypto) Fetch: liquidations, funding_rate, etc.         │
└──────────────────────────────┬──────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│ 2. VALIDATION (src/services/validation_service.py)              │
│    - Check for gaps, anomalies, missing data                    │
│    - Calculate quality_score                                     │
│    - Flag: is_validated, gap_detected, volume_anomaly          │
└──────────────────────────────┬──────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│ 3. FEATURE COMPUTATION (src/quant_engine/quant_features.py)    │
│    Universal Features (all assets):                             │
│    ├─ return_1h, return_1d                                      │
│    ├─ volatility_20, volatility_50                              │
│    ├─ atr                                                        │
│    ├─ trend_direction, market_structure                         │
│    └─ rolling_volume_20                                         │
│                                                                  │
│    Crypto Features (crypto only):                               │
│    ├─ delta, buy_sell_ratio                                     │
│    ├─ liquidation_intensity, volume_spike_score                │
│    ├─ long_short_ratio, funding_rate_percentile               │
│    ├─ exchange_inflow, open_interest_change                    │
│    └─ [Need implementation]                                     │
└──────────────────────────────┬──────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│ 4. ENRICH & STORE (src/scheduler.py: store_enriched_data)      │
│    - Combine raw data + computed features                       │
│    - Add metadata (timestamps, source, revision)                │
│    - UPSERT into market_data_v2                                │
│    - Log in enrichment_fetch_log                                │
│    - Update enrichment_status                                   │
└──────────────────────────────┬──────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│ 5. AVAILABLE FOR QUERYING (via API)                             │
│    - GET /api/v1/historical/{symbol}                            │
│    - Returns: enriched OHLCV + all computed features            │
│    - Cache: 1-hour TTL in Redis                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Key Implementation Files

| File | Purpose | Key Functions |
|------|---------|----------------|
| `src/scheduler.py` | Daily backfill orchestration | `_run_scheduler()`, `backfill_symbols()`, `store_enriched_data()` |
| `src/quant_engine/quant_features.py` | Feature computations | `compute_returns()`, `compute_volatility()`, `compute_atr()` |
| `src/services/validation_service.py` | Quality checks | `validate_data()`, `calculate_quality_score()` |
| `src/services/database_service.py` | Data persistence | `upsert_enriched_batch()`, `update_features()` |
| `database/migrations/012_add_quant_features.sql` | Feature schema | Table definitions for computed columns |

---

## Performance Notes

- **Computation Frequency:** Daily (full year historical backfill for new symbols)
- **Per-Symbol Time:** ~50-200ms (depends on data volume, feature complexity)
- **Parallel Processing:** Up to 5 symbols in parallel (configurable)
- **Storage:** ~2-3 KB per candle (49 columns of data)
- **Indexing:** Optimized on (symbol, asset_class, timeframe, timestamp)

